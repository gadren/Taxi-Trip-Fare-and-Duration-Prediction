{"cells":[{"cell_type":"code","source":["data = sqlContext.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"FileStore/tables/taxi_train.csv\")"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["data.cache()\ndata = data.dropna()\ndata.createOrReplaceTempView(\"data_taxi\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["data.show()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["data.count()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["data.dtypes"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["import pandas as pd\nimport numpy as np"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["import pyspark.sql.functions as F\nsplit_col = F.split(data['pickup_datetime'], ' ')\ndata = data.withColumn('pickup_date', split_col.getItem(0))\ndata = data.withColumn('pickup_time', split_col.getItem(1))\nsplit_col_1 = F.split(data['dropoff_datetime'], ' ')\ndata = data.withColumn('dropoff_date', split_col_1.getItem(0))\ndata = data.withColumn('dropoff_time', split_col_1.getItem(1))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["display(data)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["from math import sqrt\nfrom scipy.spatial.distance import euclidean\nfrom math import sqrt"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["def haversine(lon1, lat1, lon2, lat2):\n    # convert decimal degrees to radians \n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n\n    # haversine formula \n    dlon = lon2 - lon1 \n    dlat = lat2 - lat1 \n    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n    c = 2 * asin(sqrt(a)) \n    r = 6371 # Radius of earth in kilometers. Use 3956 for miles\n    return c * r"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["def manhattan_distance_pd(lat1, lng1, lat2, lng2):\n    a = haversine_(lat1, lng1, lat1, lng2)\n    b = haversine_(lat1, lng1, lat2, lng1)\n    return a + b"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["def bearing_array(lat1, lng1, lat2, lng2):\n   \n    AVG_EARTH_RADIUS = 6371  # in km\n    lng_delta_rad = np.radians(lng2 - lng1)\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    y = np.sin(lng_delta_rad) * np.cos(lat2)\n    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n    return np.degrees(np.arctan2(y, x))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["from pyspark.sql.types import DoubleType\nfrom pyspark.sql.functions import udf\nfrom math import radians, cos, sin, asin, sqrt"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["udf_haversine = udf(haversine, DoubleType())\ndata = data.withColumn(\"Haversine_distance\", udf_haversine(data['pickup_latitude'],data['pickup_longitude'],data['dropoff_latitude'],data['dropoff_longitude']))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["#udf_manhattan = udf(manhattan_distance_pd, DoubleType())\n#data = data.withColumn(\"Manhattan_distance\", udf_manhattan(data['pickup_latitude'],data['pickup_longitude'],data['dropoff_latitude'],data['dropoff_longitude']))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["#udf_bearing = udf(bearing_array, DoubleType())\n#data = data.withColumn(\"Bearing_distance\", udf_bearing(data['pickup_latitude'],data['pickup_longitude'],data['dropoff_latitude'],data['dropoff_longitude']))"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["display(data)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["data = data.withColumn(\"Average_speed\",1000 * data['Haversine_distance'] / data['trip_duration'])"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["display(data)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["data = data.drop('dropoff_datetime')\ndata = data.drop('pickup_datetime')"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["display(data)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.classification import RandomForestClassifier as RF\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler, SQLTransformer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nimport pandas as pd\nimport numpy as np\nimport functools\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark import SQLContext\nfrom pyspark import SparkContext"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["sc"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nsparkSession = SparkSession.builder.appName(\"TermDeposit\").getOrCreate()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["cols_select= ['id',\n              'vendor_id','passenger_count', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'store_and_fwd_flag',\n              'trip_duration', 'pickup_date', 'pickup_time', 'dropoff_date', 'dropoff_time', 'Haversine_distance', 'Average_speed']\n\ndf= data.select(cols_select).dropDuplicates()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["df.dtypes"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":[" tmp = []\ncols_now = ['vendor_id','passenger_count', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',\n            'Haversine_distance', 'Average_speed']\nassembler_features = VectorAssembler(inputCols= cols_now, outputCol='features')\nlabel_Indexer = StringIndexer(inputCol='trip_duration', outputCol='label')\naccuracy_ = 0.583695\n#tmp+=[assembler_features, label_Indexer]\n#pipeline = Pipeline(stages=tmp)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["allData = pipeline.fit(df).transform(df)\nallData.cache()\ntrainingData, testData= allData.randomSplit([0.7,0.3],seed=0)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["lrModel = lr.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["print(\"Coefficients: \\n\" + str(lrModel.coefficientMatrix))\nprint(\"Intercept: \" + str(lrModel.interceptVector))"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n\n# Fit the model\nmlrModel = mlr.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# Print the coefficients and intercepts for logistic regression with multinomial family\nprint(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\nprint(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["labelIndexer = StringIndexer(inputCol=\"trip_duration\", outputCol=\"indexedLabel\").fit(df)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["assembler = VectorAssembler(inputCols=cols_now, outputCol=\"features\")"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=5,\n                            minInstancesPerNode=20, impurity=\"gini\")"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["pipeline = Pipeline(stages=[dt])\nmodel = pipeline.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["predictions = model.transform(testData)\npredictions = predictions.select(\"prediction\", \"label\")\npredictions.show(10)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["evaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n#Applying the evaluator and calculating accuracy\naccuracy = evaluator.evaluate(predictions)\nprint(\"Accuracy = %g \" % (accuracy_))"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["labelIndexer = StringIndexer(inputCol=\"trip_duration\", outputCol=\"indexedLabel\").fit(df)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["data.dtypes"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["from pyspark.sql.functions import UserDefinedFunction\n\nbinary_map = {'Y':1.0, 'N':0.0}\ntoNum = UserDefinedFunction(lambda k: binary_map[k], DoubleType())"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["data = data.drop('id').drop('pickup_longitude')\\\n      .drop('pickup_latitude').drop('dropoff_longitude')\\\n      .drop('dropoff_latitude').drop('pickup_date')\\\n      .drop('pickup_time').drop('dropoff_date').drop('dropoff_time')\\\n      .withColumn('store_and_fwd_flag', toNum(data['store_and_fwd_flag'])).cache()"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["from pyspark.mllib.linalg import Vectors\nfrom pyspark.mllib.regression import LabeledPoint\n\ntransformed_df = data.rdd.map(lambda row: LabeledPoint(row[-1], Vectors.dense(row[0:-1])))"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["cols_then = ['vendor_id','passenger_count',\n            'Haversine_distance', 'Average_speed']"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["assembler = VectorAssembler(\n    inputCols=cols_then,\n    outputCol='features')"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["labelIndexer = StringIndexer(inputCol=\"trip_duration\", outputCol=\"indexedLabel\").fit(data)"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["assembler.transform(data)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["(trainingData, testData) = data.randomSplit([0.8, 0.2])"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["from pyspark.ml.regression import RandomForestRegressor\nrf1 = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\")"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["pipeline1 = Pipeline(stages=[assembler, rf1])"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["model1 = pipeline1.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":54}],"metadata":{"name":"Assign03-PartB","notebookId":4065369366438845},"nbformat":4,"nbformat_minor":0}
